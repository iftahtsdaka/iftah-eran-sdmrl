{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "# -----------------------\n",
    "# general\n",
    "# -----------------------\n",
    "import os\n",
    "\n",
    "# -----------------------\n",
    "# Gymnasium\n",
    "# -----------------------\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------\n",
    "# Stable Baselines 3\n",
    "# -----------------------\n",
    "\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Global Constants\n",
    "# -----------------------\n",
    "TOTAL_DAILY_DEMAND = 1000\n",
    "PENALTY_PER_WATER_UNIT = 1000\n",
    "AGENT_WATER_VOLUME_MAX = 300\n",
    "HOURS_IN_A_WEEK = 168\n",
    "\n",
    "# -----------------------\n",
    "# Helper Functions\n",
    "# -----------------------\n",
    "def discretize(value, bucket_size):\n",
    "    \"\"\"Rounds the value to the nearest multiple of bucket_size.\"\"\"\n",
    "    return int(round(value / bucket_size)) * bucket_size\n",
    "\n",
    "def default_price_function(amount_to_buy, base_price):\n",
    "    return min(amount_to_buy * base_price + 0.005 * base_price * (amount_to_buy - 1) ** 2,\n",
    "               PENALTY_PER_WATER_UNIT)\n",
    "\n",
    "def get_hourly_demand_pattern():\n",
    "    hourly_demand = np.array([2, 2, 2, 2, 3, 5, 10, 12, 10, 8, 6, 5, 5, 5, 5, 6, 7, 9, 10, 9, 6, 4, 3, 2])\n",
    "    hourly_demand = (hourly_demand / hourly_demand.sum()) * TOTAL_DAILY_DEMAND\n",
    "    # For a week: 6 full days and 1 day at half demand\n",
    "    hourly_demand = np.append(np.tile(hourly_demand, 6), hourly_demand / 2)\n",
    "    return hourly_demand\n",
    "\n",
    "def sample_demand(hour, std=10):\n",
    "    pattern = get_hourly_demand_pattern()\n",
    "    mean_demand = pattern[hour]\n",
    "    return max(0, np.random.normal(mean_demand, std))\n",
    "\n",
    "def get_water_prices(hours):\n",
    "    base_prices = np.ones(24)\n",
    "    base_prices[8:16] = 2  # More expensive between 8:00 and 16:00\n",
    "    base_prices = np.tile(base_prices, 7)[:hours]\n",
    "    return base_prices\n",
    "\n",
    "# -----------------------\n",
    "# Simplified Environment\n",
    "# -----------------------\n",
    "class SimplifiedWaterSupplyEnv(gym.Env):\n",
    "    def __init__(self,\n",
    "                 max_cycles=10,\n",
    "                 hours_per_cycle=HOURS_IN_A_WEEK,\n",
    "                 time_bucket_count=168,\n",
    "                 water_bucket_count=300,\n",
    "                 price_function=default_price_function):\n",
    "        \"\"\"\n",
    "        max_cycles: number of cycles (e.g., weeks)\n",
    "        hours_per_cycle: number of hours in one cycle (e.g., 168)\n",
    "        time_bucket_count: discrete time steps per cycle (aggregated from hours)\n",
    "        water_bucket_count: number of discrete water/demand levels\n",
    "        price_function: function for computing water cost\n",
    "        \"\"\"\n",
    "        super(SimplifiedWaterSupplyEnv, self).__init__()\n",
    "        self.max_cycles = max_cycles\n",
    "        self.hours_per_cycle = hours_per_cycle\n",
    "        self.time_bucket_count = time_bucket_count\n",
    "        self.price_function = price_function\n",
    "\n",
    "        # How many original hours per bucket?\n",
    "        self.aggregation_interval = hours_per_cycle // time_bucket_count\n",
    "\n",
    "        # Determine the bucket size for water/demand.\n",
    "        self.water_bucket_count = water_bucket_count\n",
    "        self.water_bucket_size = AGENT_WATER_VOLUME_MAX / water_bucket_count\n",
    "\n",
    "        # Observation: [water_level, price_A, price_B, demand, current_time_bucket]\n",
    "        self.observation_space = spaces.Box(low=0, high=np.inf, shape=(5,), dtype=np.float32)\n",
    "        # Action: how much water to buy from each source (continuous; later wrapped to discrete)\n",
    "        self.action_space = spaces.Box(low=0, high=AGENT_WATER_VOLUME_MAX, shape=(2,), dtype=np.float32)\n",
    "\n",
    "        # Pre-compute hourly prices and then aggregate them into time buckets.\n",
    "        self.base_hourly_prices = get_water_prices(hours_per_cycle)\n",
    "        self.source_A_base_prices = self._aggregate_prices(self.base_hourly_prices)\n",
    "        self.source_B_base_prices = 1.5 * self.source_A_base_prices\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def _aggregate_prices(self, hourly_prices):\n",
    "        \"\"\"Aggregate hourly prices into time buckets by averaging.\"\"\"\n",
    "        aggregated = []\n",
    "        for i in range(self.time_bucket_count):\n",
    "            start = i * self.aggregation_interval\n",
    "            end = (i + 1) * self.aggregation_interval\n",
    "            agg_price = np.mean(hourly_prices[start:end])\n",
    "            aggregated.append(agg_price)\n",
    "        return np.array(aggregated)\n",
    "\n",
    "    def _aggregate_demand(self, start_hour):\n",
    "        \"\"\"Aggregate demand over the time bucket and discretize it.\"\"\"\n",
    "        demands = [sample_demand(h % self.hours_per_cycle) for h in range(start_hour, start_hour + self.aggregation_interval)]\n",
    "        avg_demand = np.mean(demands)\n",
    "        return discretize(avg_demand, self.water_bucket_size)\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.current_cycle = 0\n",
    "        self.current_time_bucket = 0\n",
    "\n",
    "        # Start with full water (discretized)\n",
    "        self.water_level = discretize(AGENT_WATER_VOLUME_MAX, self.water_bucket_size)\n",
    "        self.total_reward = 0.0\n",
    "\n",
    "        # Initialize demand and prices for the first time bucket\n",
    "        self.demand = self._aggregate_demand(0)\n",
    "        self.price_A = self.source_A_base_prices[self.current_time_bucket]\n",
    "        self.price_B = self.source_B_base_prices[self.current_time_bucket]\n",
    "        return self._get_obs(), self._get_info()\n",
    "\n",
    "    def step(self, action):\n",
    "        buy_from_A, buy_from_B = action\n",
    "\n",
    "        # Calculate penalty for unmet demand\n",
    "        unmet_demand = max(0, self.demand - self.water_level)\n",
    "        unmet_demand_penalty = unmet_demand * PENALTY_PER_WATER_UNIT\n",
    "\n",
    "        # Subtract demand (and discretize afterward)\n",
    "        self.water_level = max(0, self.water_level - self.demand)\n",
    "\n",
    "        # Compute cost using the pricing function\n",
    "        cost_A = self.price_function(buy_from_A, self.price_A)\n",
    "        cost_B = self.price_function(buy_from_B, self.price_B)\n",
    "\n",
    "        # Add purchased water and re-discretize\n",
    "        self.water_level += (buy_from_A + buy_from_B)\n",
    "        self.water_level = discretize(self.water_level, self.water_bucket_size)\n",
    "\n",
    "        reward = - cost_A - cost_B - unmet_demand_penalty\n",
    "        self.total_reward += reward\n",
    "\n",
    "        # Advance the time bucket\n",
    "        self.current_time_bucket += 1\n",
    "        if self.current_time_bucket >= self.time_bucket_count:\n",
    "            self.current_cycle += 1\n",
    "            self.current_time_bucket = 0\n",
    "\n",
    "        done = self.current_cycle >= self.max_cycles\n",
    "\n",
    "        if not done:\n",
    "            start_hour = self.current_time_bucket * self.aggregation_interval\n",
    "            self.demand = self._aggregate_demand(start_hour)\n",
    "            self.price_A = self.source_A_base_prices[self.current_time_bucket]\n",
    "            self.price_B = self.source_B_base_prices[self.current_time_bucket]\n",
    "\n",
    "        return self._get_obs(), reward, done, False, self._get_info()\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return np.array([self.water_level, self.price_A, self.price_B, self.demand, self.current_time_bucket], dtype=np.float32)\n",
    "\n",
    "    def _get_info(self):\n",
    "        return {\n",
    "            \"water_level\": self.water_level,\n",
    "            \"price_A\": self.price_A,\n",
    "            \"price_B\": self.price_B,\n",
    "            \"demand\": self.demand,\n",
    "            \"current_time_bucket\": self.current_time_bucket,\n",
    "            \"current_cycle\": self.current_cycle,\n",
    "            \"total_reward\": self.total_reward\n",
    "        }\n",
    "\n",
    "    def render(self):\n",
    "        info = self._get_info()\n",
    "        print(info)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete Wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Discrete Wrappers\n",
    "# -----------------------\n",
    "class DiscreteActions(gym.ActionWrapper):\n",
    "    \"\"\"\n",
    "    Wraps a continuous action space into a discrete one.\n",
    "    The agent can buy water in increments of size_of_purchase.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, max_water_volume=AGENT_WATER_VOLUME_MAX, size_of_purchase=10):\n",
    "        super().__init__(env)\n",
    "        self.action_amount = max_water_volume // size_of_purchase + 1\n",
    "        self.size_of_purchase = size_of_purchase\n",
    "        # Flattened action space: two sources => action_amount^2 possible actions.\n",
    "        self.action_space = spaces.Discrete(self.action_amount * self.action_amount)\n",
    "\n",
    "    def action_to_quantity(self, action):\n",
    "        from_source_1 = action // self.action_amount\n",
    "        from_source_2 = action % self.action_amount\n",
    "        return [from_source_1 * self.size_of_purchase, from_source_2 * self.size_of_purchase]\n",
    "\n",
    "    def action(self, action):\n",
    "        return self.action_to_quantity(action)\n",
    "\n",
    "class DiscreteObservation(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Wraps the observation space into a single discrete index.\n",
    "    Each component is discretized according to the provided resolutions.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, water_level_resolution=10, price_resolution=1, demand_resolution=10, time_resolution=1):\n",
    "        super().__init__(env)\n",
    "        self.water_level_resolution = water_level_resolution\n",
    "        self.price_resolution = price_resolution\n",
    "        self.demand_resolution = demand_resolution\n",
    "        self.time_resolution = time_resolution\n",
    "\n",
    "        self.amount_of_water = AGENT_WATER_VOLUME_MAX // water_level_resolution + 1\n",
    "        self.amount_of_price = PENALTY_PER_WATER_UNIT // price_resolution + 1\n",
    "        self.amount_of_demand = TOTAL_DAILY_DEMAND // demand_resolution + 1\n",
    "        self.amount_of_time = self.env.time_bucket_count\n",
    "\n",
    "        # The flattened observation index\n",
    "        self.observation_space = spaces.Discrete(self.amount_of_water * \n",
    "                                                 self.amount_of_price * \n",
    "                                                 self.amount_of_price * \n",
    "                                                 self.amount_of_demand * \n",
    "                                                 self.amount_of_time)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        # observation: [water_level, price_A, price_B, demand, current_time_bucket]\n",
    "        water_level, price_A, price_B, demand, current_time_bucket = observation\n",
    "\n",
    "        water_idx = int(water_level // self.water_level_resolution)\n",
    "        price_A_idx = int(price_A // self.price_resolution)\n",
    "        price_B_idx = int(price_B // self.price_resolution)\n",
    "        demand_idx = int(demand // self.demand_resolution)\n",
    "        time_idx = int(current_time_bucket // self.time_resolution)\n",
    "\n",
    "        discrete_obs = (\n",
    "            water_idx +\n",
    "            self.amount_of_water * (\n",
    "                price_A_idx +\n",
    "                self.amount_of_price * (\n",
    "                    price_B_idx +\n",
    "                    self.amount_of_price * (\n",
    "                        demand_idx +\n",
    "                        self.amount_of_demand * time_idx\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        return discrete_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------\n",
    "# Environment Creation Function\n",
    "# -----------------------\n",
    "def create_env(time_buckets=5, water_buckets=5):\n",
    "    \"\"\"\n",
    "    time_buckets: number of discrete time steps per cycle (also hours_per_cycle).\n",
    "    water_buckets: number of discrete water levels (e.g., 5 levels).\n",
    "    \"\"\"\n",
    "    env = SimplifiedWaterSupplyEnv(\n",
    "        max_cycles=5,\n",
    "        hours_per_cycle=time_buckets,          # each bucket represents one hour\n",
    "        time_bucket_count=time_buckets,        # number of time steps equals time_buckets\n",
    "        water_bucket_count=water_buckets - 1   # water_bucket_count divisions yield water_buckets levels\n",
    "    )\n",
    "    # Determine the purchase increment: AGENT_WATER_VOLUME_MAX divided by (water_buckets - 1)\n",
    "    purchase_increment = AGENT_WATER_VOLUME_MAX // (water_buckets - 1)\n",
    "    \n",
    "    env = DiscreteActions(env, max_water_volume=AGENT_WATER_VOLUME_MAX, size_of_purchase=purchase_increment)\n",
    "    env = DiscreteObservation(\n",
    "        env,\n",
    "        water_level_resolution=purchase_increment,  # This yields 5 water levels: 0,75,150,225,300\n",
    "        price_resolution=250,                         # 1000//250 + 1 = 5 price buckets\n",
    "        demand_resolution=250,                        # 1000//250 + 1 = 5 demand buckets\n",
    "        time_resolution=1                             # each time bucket is one step\n",
    "    )\n",
    "    return env\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 25        |\n",
      "|    ep_rew_mean      | -1.95e+04 |\n",
      "|    exploration_rate | 0.81      |\n",
      "| time/               |           |\n",
      "|    episodes         | 4         |\n",
      "|    fps              | 5554      |\n",
      "|    time_elapsed     | 0         |\n",
      "|    total_timesteps  | 100       |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 25        |\n",
      "|    ep_rew_mean      | -2.02e+04 |\n",
      "|    exploration_rate | 0.62      |\n",
      "| time/               |           |\n",
      "|    episodes         | 8         |\n",
      "|    fps              | 5127      |\n",
      "|    time_elapsed     | 0         |\n",
      "|    total_timesteps  | 200       |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 25        |\n",
      "|    ep_rew_mean      | -2.02e+04 |\n",
      "|    exploration_rate | 0.43      |\n",
      "| time/               |           |\n",
      "|    episodes         | 12        |\n",
      "|    fps              | 5454      |\n",
      "|    time_elapsed     | 0         |\n",
      "|    total_timesteps  | 300       |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 25        |\n",
      "|    ep_rew_mean      | -2.02e+04 |\n",
      "|    exploration_rate | 0.24      |\n",
      "| time/               |           |\n",
      "|    episodes         | 16        |\n",
      "|    fps              | 5744      |\n",
      "|    time_elapsed     | 0         |\n",
      "|    total_timesteps  | 400       |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 25        |\n",
      "|    ep_rew_mean      | -1.98e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 20        |\n",
      "|    fps              | 5847      |\n",
      "|    time_elapsed     | 0         |\n",
      "|    total_timesteps  | 500       |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 25        |\n",
      "|    ep_rew_mean      | -1.96e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 24        |\n",
      "|    fps              | 5852      |\n",
      "|    time_elapsed     | 0         |\n",
      "|    total_timesteps  | 600       |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 25        |\n",
      "|    ep_rew_mean      | -1.95e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 28        |\n",
      "|    fps              | 6014      |\n",
      "|    time_elapsed     | 0         |\n",
      "|    total_timesteps  | 700       |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 25        |\n",
      "|    ep_rew_mean      | -1.93e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 32        |\n",
      "|    fps              | 6135      |\n",
      "|    time_elapsed     | 0         |\n",
      "|    total_timesteps  | 800       |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 25        |\n",
      "|    ep_rew_mean      | -1.92e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 36        |\n",
      "|    fps              | 6233      |\n",
      "|    time_elapsed     | 0         |\n",
      "|    total_timesteps  | 900       |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 25        |\n",
      "|    ep_rew_mean      | -1.92e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 40        |\n",
      "|    fps              | 6176      |\n",
      "|    time_elapsed     | 0         |\n",
      "|    total_timesteps  | 1000      |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eranw\\miniconda3\\envs\\filo\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:78: UserWarning: The `render_mode` attribute is not defined in your environment. It will be set to None.\n",
      "  warnings.warn(\"The `render_mode` attribute is not defined in your environment. It will be set to None.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 25        |\n",
      "|    ep_rew_mean      | -1.96e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 44        |\n",
      "|    fps              | 3203      |\n",
      "|    time_elapsed     | 0         |\n",
      "|    total_timesteps  | 1100      |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.001     |\n",
      "|    loss             | 741       |\n",
      "|    n_updates        | 24        |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 25        |\n",
      "|    ep_rew_mean      | -1.97e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 48        |\n",
      "|    fps              | 2482      |\n",
      "|    time_elapsed     | 0         |\n",
      "|    total_timesteps  | 1200      |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.001     |\n",
      "|    loss             | 731       |\n",
      "|    n_updates        | 49        |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 25        |\n",
      "|    ep_rew_mean      | -1.98e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 52        |\n",
      "|    fps              | 2086      |\n",
      "|    time_elapsed     | 0         |\n",
      "|    total_timesteps  | 1300      |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.001     |\n",
      "|    loss             | 1.59e+03  |\n",
      "|    n_updates        | 74        |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 25        |\n",
      "|    ep_rew_mean      | -1.84e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 56        |\n",
      "|    fps              | 1830      |\n",
      "|    time_elapsed     | 0         |\n",
      "|    total_timesteps  | 1400      |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.001     |\n",
      "|    loss             | 0.000188  |\n",
      "|    n_updates        | 99        |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 25        |\n",
      "|    ep_rew_mean      | -1.73e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 60        |\n",
      "|    fps              | 1658      |\n",
      "|    time_elapsed     | 0         |\n",
      "|    total_timesteps  | 1500      |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.001     |\n",
      "|    loss             | 39.4      |\n",
      "|    n_updates        | 124       |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 25        |\n",
      "|    ep_rew_mean      | -1.63e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 64        |\n",
      "|    fps              | 1524      |\n",
      "|    time_elapsed     | 1         |\n",
      "|    total_timesteps  | 1600      |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.001     |\n",
      "|    loss             | 1.25e-05  |\n",
      "|    n_updates        | 149       |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 25        |\n",
      "|    ep_rew_mean      | -1.54e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 68        |\n",
      "|    fps              | 1415      |\n",
      "|    time_elapsed     | 1         |\n",
      "|    total_timesteps  | 1700      |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.001     |\n",
      "|    loss             | 3.22e-05  |\n",
      "|    n_updates        | 174       |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 25        |\n",
      "|    ep_rew_mean      | -1.46e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 72        |\n",
      "|    fps              | 1334      |\n",
      "|    time_elapsed     | 1         |\n",
      "|    total_timesteps  | 1800      |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.001     |\n",
      "|    loss             | 6.05e-05  |\n",
      "|    n_updates        | 199       |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 25        |\n",
      "|    ep_rew_mean      | -1.39e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 76        |\n",
      "|    fps              | 1268      |\n",
      "|    time_elapsed     | 1         |\n",
      "|    total_timesteps  | 1900      |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.001     |\n",
      "|    loss             | 111       |\n",
      "|    n_updates        | 224       |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 25        |\n",
      "|    ep_rew_mean      | -1.33e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 80        |\n",
      "|    fps              | 1220      |\n",
      "|    time_elapsed     | 1         |\n",
      "|    total_timesteps  | 2000      |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.001     |\n",
      "|    loss             | 142       |\n",
      "|    n_updates        | 249       |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 25        |\n",
      "|    ep_rew_mean      | -1.27e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 84        |\n",
      "|    fps              | 1177      |\n",
      "|    time_elapsed     | 1         |\n",
      "|    total_timesteps  | 2100      |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.001     |\n",
      "|    loss             | 118       |\n",
      "|    n_updates        | 274       |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 25        |\n",
      "|    ep_rew_mean      | -1.22e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 88        |\n",
      "|    fps              | 1140      |\n",
      "|    time_elapsed     | 1         |\n",
      "|    total_timesteps  | 2200      |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.001     |\n",
      "|    loss             | 0.000251  |\n",
      "|    n_updates        | 299       |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 25        |\n",
      "|    ep_rew_mean      | -1.17e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 92        |\n",
      "|    fps              | 1102      |\n",
      "|    time_elapsed     | 2         |\n",
      "|    total_timesteps  | 2300      |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.001     |\n",
      "|    loss             | 0.000126  |\n",
      "|    n_updates        | 324       |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 25        |\n",
      "|    ep_rew_mean      | -1.13e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 96        |\n",
      "|    fps              | 1068      |\n",
      "|    time_elapsed     | 2         |\n",
      "|    total_timesteps  | 2400      |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.001     |\n",
      "|    loss             | 178       |\n",
      "|    n_updates        | 349       |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 25        |\n",
      "|    ep_rew_mean      | -1.09e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 100       |\n",
      "|    fps              | 1044      |\n",
      "|    time_elapsed     | 2         |\n",
      "|    total_timesteps  | 2500      |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.001     |\n",
      "|    loss             | 137       |\n",
      "|    n_updates        | 374       |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 25        |\n",
      "|    ep_rew_mean      | -1.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 104       |\n",
      "|    fps              | 1022      |\n",
      "|    time_elapsed     | 2         |\n",
      "|    total_timesteps  | 2600      |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.001     |\n",
      "|    loss             | 2e-05     |\n",
      "|    n_updates        | 399       |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 25        |\n",
      "|    ep_rew_mean      | -9.34e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 108       |\n",
      "|    fps              | 999       |\n",
      "|    time_elapsed     | 2         |\n",
      "|    total_timesteps  | 2700      |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.001     |\n",
      "|    loss             | 0.000287  |\n",
      "|    n_updates        | 424       |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 25        |\n",
      "|    ep_rew_mean      | -8.58e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 112       |\n",
      "|    fps              | 979       |\n",
      "|    time_elapsed     | 2         |\n",
      "|    total_timesteps  | 2800      |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.001     |\n",
      "|    loss             | 0.000273  |\n",
      "|    n_updates        | 449       |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 25        |\n",
      "|    ep_rew_mean      | -7.82e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 116       |\n",
      "|    fps              | 964       |\n",
      "|    time_elapsed     | 3         |\n",
      "|    total_timesteps  | 2900      |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.001     |\n",
      "|    loss             | 236       |\n",
      "|    n_updates        | 474       |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 25        |\n",
      "|    ep_rew_mean      | -7.11e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 120       |\n",
      "|    fps              | 949       |\n",
      "|    time_elapsed     | 3         |\n",
      "|    total_timesteps  | 3000      |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.001     |\n",
      "|    loss             | 7.81e-05  |\n",
      "|    n_updates        | 499       |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 25        |\n",
      "|    ep_rew_mean      | -6.43e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 124       |\n",
      "|    fps              | 928       |\n",
      "|    time_elapsed     | 3         |\n",
      "|    total_timesteps  | 3100      |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.001     |\n",
      "|    loss             | 210       |\n",
      "|    n_updates        | 524       |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 25        |\n",
      "|    ep_rew_mean      | -5.71e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 128       |\n",
      "|    fps              | 916       |\n",
      "|    time_elapsed     | 3         |\n",
      "|    total_timesteps  | 3200      |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.001     |\n",
      "|    loss             | 117       |\n",
      "|    n_updates        | 549       |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 25        |\n",
      "|    ep_rew_mean      | -5.01e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 132       |\n",
      "|    fps              | 901       |\n",
      "|    time_elapsed     | 3         |\n",
      "|    total_timesteps  | 3300      |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.001     |\n",
      "|    loss             | 0.00018   |\n",
      "|    n_updates        | 574       |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25       |\n",
      "|    ep_rew_mean      | -4.3e+03 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 136      |\n",
      "|    fps              | 893      |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 3400     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000215 |\n",
      "|    n_updates        | 599      |\n",
      "----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 25        |\n",
      "|    ep_rew_mean      | -3.56e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 140       |\n",
      "|    fps              | 879       |\n",
      "|    time_elapsed     | 3         |\n",
      "|    total_timesteps  | 3500      |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.001     |\n",
      "|    loss             | 0.000379  |\n",
      "|    n_updates        | 624       |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 25        |\n",
      "|    ep_rew_mean      | -2.65e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 144       |\n",
      "|    fps              | 872       |\n",
      "|    time_elapsed     | 4         |\n",
      "|    total_timesteps  | 3600      |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.001     |\n",
      "|    loss             | 0.000196  |\n",
      "|    n_updates        | 649       |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 25        |\n",
      "|    ep_rew_mean      | -1.81e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 148       |\n",
      "|    fps              | 860       |\n",
      "|    time_elapsed     | 4         |\n",
      "|    total_timesteps  | 3700      |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.001     |\n",
      "|    loss             | 22.6      |\n",
      "|    n_updates        | 674       |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 25        |\n",
      "|    ep_rew_mean      | -1.04e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 152       |\n",
      "|    fps              | 847       |\n",
      "|    time_elapsed     | 4         |\n",
      "|    total_timesteps  | 3800      |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.001     |\n",
      "|    loss             | 98.5      |\n",
      "|    n_updates        | 699       |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 25        |\n",
      "|    ep_rew_mean      | -1.03e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 156       |\n",
      "|    fps              | 829       |\n",
      "|    time_elapsed     | 4         |\n",
      "|    total_timesteps  | 3900      |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.001     |\n",
      "|    loss             | 0.000489  |\n",
      "|    n_updates        | 724       |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 25        |\n",
      "|    ep_rew_mean      | -1.03e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 160       |\n",
      "|    fps              | 824       |\n",
      "|    time_elapsed     | 4         |\n",
      "|    total_timesteps  | 4000      |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.001     |\n",
      "|    loss             | 66.1      |\n",
      "|    n_updates        | 749       |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 25        |\n",
      "|    ep_rew_mean      | -1.02e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 164       |\n",
      "|    fps              | 818       |\n",
      "|    time_elapsed     | 5         |\n",
      "|    total_timesteps  | 4100      |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.001     |\n",
      "|    loss             | 0.00043   |\n",
      "|    n_updates        | 774       |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 25        |\n",
      "|    ep_rew_mean      | -1.02e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 168       |\n",
      "|    fps              | 811       |\n",
      "|    time_elapsed     | 5         |\n",
      "|    total_timesteps  | 4200      |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.001     |\n",
      "|    loss             | 0.000414  |\n",
      "|    n_updates        | 799       |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25       |\n",
      "|    ep_rew_mean      | -993     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 172      |\n",
      "|    fps              | 806      |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 4300     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000128 |\n",
      "|    n_updates        | 824      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25       |\n",
      "|    ep_rew_mean      | -980     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 176      |\n",
      "|    fps              | 801      |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 4400     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 36.4     |\n",
      "|    n_updates        | 849      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25       |\n",
      "|    ep_rew_mean      | -941     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 180      |\n",
      "|    fps              | 795      |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 4500     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 154      |\n",
      "|    n_updates        | 874      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25       |\n",
      "|    ep_rew_mean      | -917     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 184      |\n",
      "|    fps              | 792      |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 4600     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00341  |\n",
      "|    n_updates        | 899      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25       |\n",
      "|    ep_rew_mean      | -882     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 188      |\n",
      "|    fps              | 788      |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 4700     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 71       |\n",
      "|    n_updates        | 924      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25       |\n",
      "|    ep_rew_mean      | -910     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 192      |\n",
      "|    fps              | 785      |\n",
      "|    time_elapsed     | 6        |\n",
      "|    total_timesteps  | 4800     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 20.3     |\n",
      "|    n_updates        | 949      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25       |\n",
      "|    ep_rew_mean      | -894     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 196      |\n",
      "|    fps              | 782      |\n",
      "|    time_elapsed     | 6        |\n",
      "|    total_timesteps  | 4900     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0308   |\n",
      "|    n_updates        | 974      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25       |\n",
      "|    ep_rew_mean      | -875     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 200      |\n",
      "|    fps              | 780      |\n",
      "|    time_elapsed     | 6        |\n",
      "|    total_timesteps  | 5000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 104      |\n",
      "|    n_updates        | 999      |\n",
      "----------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdqn_water_supply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Evaluate the model over 10 episodes\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m mean_reward, std_reward \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_eval_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_reward\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m +/- \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstd_reward\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\eranw\\miniconda3\\envs\\filo\\Lib\\site-packages\\stable_baselines3\\common\\evaluation.py:97\u001b[0m, in \u001b[0;36mevaluate_policy\u001b[1;34m(model, env, n_eval_episodes, deterministic, render, callback, reward_threshold, return_episode_rewards, warn)\u001b[0m\n\u001b[0;32m     95\u001b[0m current_rewards \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m rewards\n\u001b[0;32m     96\u001b[0m current_lengths \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 97\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn_envs\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m episode_counts[i] \u001b[38;5;241m<\u001b[39m episode_count_targets[i]:\n\u001b[0;32m     99\u001b[0m         \u001b[38;5;66;03m# unpack values so that the callback can access the local variables\u001b[39;00m\n\u001b[0;32m    100\u001b[0m         reward \u001b[38;5;241m=\u001b[39m rewards[i]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a vectorized environment (n_envs=1 for simplicity)\n",
    "env = make_vec_env(lambda: create_env(time_buckets=5, water_buckets=5), n_envs=1)\n",
    "# Create and configure the DQN model\n",
    "model = DQN(\"MlpPolicy\", env, verbose=1, learning_rate=1e-3,\n",
    "            buffer_size=10, learning_starts=1000, exploration_fraction=0.1)\n",
    "\n",
    "# Train the model\n",
    "model.learn(total_timesteps=5000)\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"dqn_water_supply\")\n",
    "\n",
    "# Evaluate the model over 10 episodes\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "print(f\"Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check environemnt\n",
    "Accordance with Gymnasium standards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed successfully.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "# Define a set of expected keys from env.info()\n",
    "EXPECTED_INFO_KEYS = {\n",
    "    \"water_level\",\n",
    "    \"price_A\",\n",
    "    \"price_B\",\n",
    "    \"demand\",\n",
    "    \"current_time_bucket\",\n",
    "    \"current_cycle\",\n",
    "    \"total_reward\",\n",
    "}\n",
    "\n",
    "def test_reset_environment():\n",
    "    \"\"\"Test that the environment resets properly.\"\"\"\n",
    "    # Create environment with a couple of cycles for a quick test\n",
    "    env = SimplifiedWaterSupplyEnv(max_cycles=2, hours_per_cycle=168,\n",
    "                                   time_bucket_count=168, water_bucket_count=300)\n",
    "    obs, info = env.reset()\n",
    "    \n",
    "    # Observation should be a numpy array with shape (5,)\n",
    "    assert isinstance(obs, np.ndarray), \"Observation is not a numpy array.\"\n",
    "    assert obs.shape == (5,), f\"Expected observation shape (5,), got {obs.shape}\"\n",
    "    \n",
    "    # Info should include all expected keys\n",
    "    for key in EXPECTED_INFO_KEYS:\n",
    "        assert key in info, f\"Missing key '{key}' in info dictionary.\"\n",
    "\n",
    "def test_step_environment():\n",
    "    \"\"\"Test a single step of the environment.\"\"\"\n",
    "    env = SimplifiedWaterSupplyEnv(max_cycles=2, hours_per_cycle=168,\n",
    "                                   time_bucket_count=168, water_bucket_count=300)\n",
    "    obs, info = env.reset()\n",
    "    \n",
    "    # Use a sample continuous action (example: buy 10 units from each source)\n",
    "    action = np.array([10.0, 10.0])\n",
    "    new_obs, reward, done, truncated, info = env.step(action)\n",
    "    \n",
    "    # Check observation shape and type\n",
    "    assert isinstance(new_obs, np.ndarray), \"New observation is not a numpy array.\"\n",
    "    assert new_obs.shape == (5,), f\"Expected new_obs shape (5,), got {new_obs.shape}\"\n",
    "    \n",
    "    # Check reward and termination flags\n",
    "    assert isinstance(reward, float), \"Reward is not a float.\"\n",
    "    assert isinstance(done, bool), \"Done flag is not boolean.\"\n",
    "    assert isinstance(truncated, bool), \"Truncated flag is not boolean.\"\n",
    "    \n",
    "    # Check info dictionary again\n",
    "    for key in EXPECTED_INFO_KEYS:\n",
    "        assert key in info, f\"Missing key '{key}' in info dictionary after step.\"\n",
    "\n",
    "def test_env_with_check_env():\n",
    "    \"\"\"Run gym's check_env to ensure the environment conforms to the Gym API.\"\"\"\n",
    "    env = SimplifiedWaterSupplyEnv()\n",
    "    # This will raise an error if the environment does not follow the Gym API.\n",
    "    check_env(env)\n",
    "\n",
    "def test_discrete_actions_wrapper():\n",
    "    \"\"\"Test the DiscreteActions wrapper functionality.\"\"\"\n",
    "    env = SimplifiedWaterSupplyEnv()\n",
    "    # Wrap the env to get discrete actions (e.g., actions in increments of 10)\n",
    "    wrapped_env = DiscreteActions(env, max_water_volume=300, size_of_purchase=10)\n",
    "    \n",
    "    # Check that the action_space is now Discrete\n",
    "    assert isinstance(wrapped_env.action_space, gym.spaces.Discrete), \\\n",
    "        \"Action space is not Discrete after wrapping.\"\n",
    "    \n",
    "    # Test conversion: for a given discrete action index, the wrapper should return a list with two values\n",
    "    discrete_index = 5  # example discrete action index\n",
    "    continuous_action = wrapped_env.action(discrete_index)\n",
    "    assert isinstance(continuous_action, list), \"Returned action is not a list.\"\n",
    "    assert len(continuous_action) == 2, \"Expected two actions (one per source).\"\n",
    "    \n",
    "    # Check that each action quantity is a multiple of the size_of_purchase (10)\n",
    "    for q in continuous_action:\n",
    "        assert q % 10 == 0, f\"Action {q} is not a multiple of 10.\"\n",
    "\n",
    "def test_discrete_observation_wrapper():\n",
    "    \"\"\"Test the DiscreteObservation wrapper functionality.\"\"\"\n",
    "    env = SimplifiedWaterSupplyEnv()\n",
    "    # Wrap the env so that observations are converted to a single discrete index.\n",
    "    wrapped_env = DiscreteObservation(env, water_level_resolution=10,\n",
    "                                      price_resolution=1,\n",
    "                                      demand_resolution=10,\n",
    "                                      time_resolution=1)\n",
    "    \n",
    "    # Check that the observation_space is now Discrete\n",
    "    assert isinstance(wrapped_env.observation_space, gym.spaces.Discrete), \\\n",
    "        \"Observation space is not Discrete after wrapping.\"\n",
    "    \n",
    "    # Reset the base env and then transform the observation\n",
    "    obs, _ = env.reset()\n",
    "    discrete_obs = wrapped_env.observation(obs)\n",
    "    # For discrete wrappers, the observation is typically an integer index.\n",
    "    assert isinstance(discrete_obs, int), \"Discrete observation is not an integer.\"\n",
    "\n",
    "\n",
    "test_reset_environment()\n",
    "test_step_environment()\n",
    "test_env_with_check_env()\n",
    "test_discrete_actions_wrapper()\n",
    "test_discrete_observation_wrapper()\n",
    "print(\"All tests passed successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a few test episodes to observe behavior\n",
    "test_env = create_env()\n",
    "obs, info = test_env.reset()\n",
    "for _ in range(20):\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, _, info = test_env.step(action)\n",
    "    test_env.render()\n",
    "    if done:\n",
    "        obs, info = test_env.reset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_vec_env(lambda: create_env(time_buckets=5, water_buckets=5), n_envs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs size: 3125\n",
      "act size: 25\n",
      "--------------------------------------------------\n",
      "obs: \n",
      "Discrete(3125)\n",
      "act: \n",
      "Discrete(25)\n"
     ]
    }
   ],
   "source": [
    "print(f\"obs size: {env.observation_space.n}\")\n",
    "print(f\"act size: {env.action_space.n}\")\n",
    "print(f\"-\" * 50)\n",
    "print(f\"obs: \\n{env.observation_space}\")\n",
    "print(f\"act: \\n{env.action_space}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
